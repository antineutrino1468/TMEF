{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.linear_model import LassoCV\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendall accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall_acc(x, y, percentage=True):\n",
    "    tau, _ = kendalltau(x, y)\n",
    "    kt_acc = 0.5 + tau / 2\n",
    "    n = len(x)\n",
    "    kt_se = np.sqrt((kt_acc * (1 - kt_acc)) / n)\n",
    "    lower = kt_acc - 1.96 * kt_se\n",
    "    upper = kt_acc + 1.96 * kt_se\n",
    "    report = pd.DataFrame({\n",
    "        \"acc\": [kt_acc],\n",
    "        \"lower\": [lower],\n",
    "        \"upper\": [upper]\n",
    "    }).round(4)\n",
    "    if percentage:\n",
    "        report *= 100\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review text processing funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean useless content in the text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[^\\s]*', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# stem the words in the text\n",
    "ps = PorterStemmer()\n",
    "def stem_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Users Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Game Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Clean_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wolfenstein: The Old Blood</td>\n",
       "      <td>Decided to play some culturally and historical...</td>\n",
       "      <td>70</td>\n",
       "      <td>decid to play some cultur and histor relev med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Wolfenstein: The Old Blood</td>\n",
       "      <td>if you are very hungry for more neo-wolfenstei...</td>\n",
       "      <td>70</td>\n",
       "      <td>if you are veri hungri for more neowolfenstein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Wolfenstein: The Old Blood</td>\n",
       "      <td>Slightly less fun version of the New Order</td>\n",
       "      <td>60</td>\n",
       "      <td>slightli less fun version of the new order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wolfenstein: The Old Blood</td>\n",
       "      <td>Part 1 is a tad of a slog by comparison with p...</td>\n",
       "      <td>70</td>\n",
       "      <td>part is a tad of a slog by comparison with par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wolfenstein: The Old Blood</td>\n",
       "      <td>This was a relatively short game. I beat the g...</td>\n",
       "      <td>60</td>\n",
       "      <td>thi wa a rel short game i beat the game on ube...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   Game Name  \\\n",
       "0           0  Wolfenstein: The Old Blood   \n",
       "1           1  Wolfenstein: The Old Blood   \n",
       "2           2  Wolfenstein: The Old Blood   \n",
       "3           3  Wolfenstein: The Old Blood   \n",
       "4           4  Wolfenstein: The Old Blood   \n",
       "\n",
       "                                              Review  Rating  \\\n",
       "0  Decided to play some culturally and historical...      70   \n",
       "1  if you are very hungry for more neo-wolfenstei...      70   \n",
       "2         Slightly less fun version of the New Order      60   \n",
       "3  Part 1 is a tad of a slog by comparison with p...      70   \n",
       "4  This was a relatively short game. I beat the g...      60   \n",
       "\n",
       "                                        Clean_Review  \n",
       "0  decid to play some cultur and histor relev med...  \n",
       "1  if you are veri hungri for more neowolfenstein...  \n",
       "2         slightli less fun version of the new order  \n",
       "3  part is a tad of a slog by comparison with par...  \n",
       "4  thi wa a rel short game i beat the game on ube...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = pd.read_csv(\"./user_df_filtered_nplus.csv\")\n",
    "\n",
    "# clean review and stem\n",
    "user_df[\"Clean_Review\"] = user_df[\"Review\"].apply(clean_text)\n",
    "user_df[\"Clean_Review\"] = user_df[\"Clean_Review\"].apply(stem_text)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply users model to users review\n",
    "To draw the coefficient plot on users model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split date to test and train set\n",
    "X_temp, _, y_train, y_test = train_test_split(user_df[[\"Clean_Review\"]],\n",
    "                                              user_df[\"Rating\"],\n",
    "                                              test_size=0.2,\n",
    "                                              random_state=42)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train_normalized = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_normalized = scaler_y.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tfidf features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, \n",
    "                                   ngram_range=(1, 3), \n",
    "                                   stop_words='english', \n",
    "                                   min_df=5, \n",
    "                                   max_df=0.95)\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_temp[\"Clean_Review\"])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(user_df.loc[y_test.index, \"Clean_Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ngram features\n",
    "ngram_vectorizer = CountVectorizer(max_features=5000, \n",
    "                                   ngram_range=(1, 3), \n",
    "                                   stop_words='english', \n",
    "                                   min_df=5, \n",
    "                                   max_df=0.95)\n",
    "X_ngrams_train = ngram_vectorizer.fit_transform(X_temp[\"Clean_Review\"])\n",
    "X_ngrams_test = ngram_vectorizer.transform(user_df.loc[y_test.index, \"Clean_Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\34233\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load lasso model\n",
    "lasso = pickle.load(open('lasso_model_user.sav', 'rb'))\n",
    "\n",
    "# get the feature names\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_names_ngrams = ngram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# get important features (with nonzero coefficient)\n",
    "important_features_users = []\n",
    "for idx, coef in enumerate(lasso.coef_):\n",
    "    if abs(coef) > 0:\n",
    "        if idx < len(feature_names_tfidf):\n",
    "            important_features_users.append((feature_names_tfidf[idx], coef))\n",
    "        elif idx < len(feature_names_tfidf)  + len(feature_names_ngrams):\n",
    "            important_features_users.append((feature_names_ngrams[idx - len(feature_names_tfidf)], coef))\n",
    "        else:\n",
    "            important_features_users.append((f\"Embedding_{idx - len(feature_names_tfidf)  - len(feature_names_ngrams)}\", coef))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coefficient plot of users\n",
    "Here, we just merge to get the coefficient-frequency data.\n",
    "The plot is done by R (coefficent_plot.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  importance  frequency\n",
      "0         abandon   -0.001038   0.004019\n",
      "1            abov    0.000400   0.006402\n",
      "2         absolut    0.001268   0.048148\n",
      "3  absolut incred    0.000335   0.000000\n",
      "4    absolut love    0.000110   0.000000\n"
     ]
    }
   ],
   "source": [
    "# get all the features name\n",
    "all_feature_names_train = np.unique(np.concatenate([feature_names_tfidf, feature_names_ngrams]))\n",
    "vectorizer = CountVectorizer(vocabulary=all_feature_names_train)\n",
    "X_all_features = vectorizer.fit_transform(X_temp[\"Clean_Review\"])\n",
    "\n",
    "# calculate the features frequency\n",
    "word_doc_frequency = (X_all_features > 0).sum(axis=0) / X_all_features.shape[0]\n",
    "\n",
    "# convert frequency data to DataFrame\n",
    "word_freq_df = pd.DataFrame({\"word\": all_feature_names_train, \"doc_frequency\": np.array(word_doc_frequency).flatten()})\n",
    "word_freq_df = word_freq_df.sort_values(by=\"doc_frequency\", ascending=False)\n",
    "\n",
    "# merge the coefficient date to the frequency data\n",
    "important_features_df = pd.DataFrame(important_features_users, columns=[\"word\", \"importance\"])\n",
    "word_freq_df = word_freq_df.rename(columns={\"doc_frequency\": \"frequency\"})\n",
    "freq_coef_user = pd.merge(important_features_df, word_freq_df, on=\"word\", how=\"inner\")\n",
    "\n",
    "# save merged data\n",
    "freq_coef_user.to_csv(\"freq_coef_user.csv\", index=False)\n",
    "print(freq_coef_user.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply users model to Media review\n",
    "Load media data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_df = pd.read_csv(\"myMediaReviews.csv\")\n",
    "media_df.dropna(subset=[\"Snippet\"], inplace=True)\n",
    "media_df.drop(columns=[\"OpenCritic URL\",\"Description\",\"Release Date\",\"Review Title\",\"Published Date\",\"Review URL\",\"Language\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter media review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34233\\AppData\\Local\\Temp\\ipykernel_12584\\1782862682.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  media_df_filtered.drop(columns=[\"Tier\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Select common games of users and media\n",
    "media_games = media_df[\"Game\"].unique()\n",
    "user_games = user_df[\"Game Name\"].unique()\n",
    "common_games = set(media_games) & set(user_games)\n",
    "media_df_filtered = media_df[media_df[\"Game\"].isin(user_games)]\n",
    "media_df_filtered[\"Game\"].nunique()\n",
    "media_df_filtered.drop(columns=[\"Tier\"], inplace=True)\n",
    "\n",
    "# Select games with review number over than 50\n",
    "n = 50\n",
    "game_counts = media_df_filtered[\"Game\"].value_counts()\n",
    "games_with_n_or_more_reviews = game_counts[game_counts >= n].index\n",
    "media_df_filtered_nplus = media_df_filtered[media_df_filtered[\"Game\"].isin(games_with_n_or_more_reviews)]\n",
    "media_df_filtered_nplus.nunique()\n",
    "media_df_filtered_nplus.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Clean the media review text\n",
    "media_df = media_df_filtered_nplus\n",
    "media_df.dropna(subset=[\"Snippet\"], inplace=True)\n",
    "media_df[\"Clean_Snippet\"] = media_df[\"Snippet\"].apply(clean_text)\n",
    "media_df[\"Clean_Snippet\"] = media_df[\"Clean_Snippet\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the X(features of text) and Y(media score)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized Y\n",
    "y_media = media_df[\"Score\"].values\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_media_normalized = scaler_y.fit_transform(y_media.reshape(-1, 1))\n",
    "\n",
    "# get features\n",
    "\n",
    "# tfidf\n",
    "X_tfidf_media = tfidf_vectorizer.transform(media_df[\"Clean_Snippet\"])\n",
    "# ngrams\n",
    "X_ngrams_media = ngram_vectorizer.transform(media_df[\"Clean_Snippet\"])\n",
    "# embedding\n",
    "def tokenize_reviews(reviews):\n",
    "    return [simple_preprocess(review) for review in reviews]\n",
    "tokenized_reviews = tokenize_reviews(user_df[\"Clean_Review\"].tolist())\n",
    "\n",
    "def get_sentence_embedding(review, model, vector_size=100):\n",
    "    words = simple_preprocess(review)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_reviews, \n",
    "                 vector_size=100,\n",
    "                 window=5, \n",
    "                 min_count=5,\n",
    "                 workers=4)\n",
    "\n",
    "batch_size = 10000\n",
    "embeddings_media = []\n",
    "for i in range(0, len(media_df), batch_size):\n",
    "    batch_reviews = media_df[\"Clean_Snippet\"].iloc[i:i + batch_size].tolist()\n",
    "    batch_embeddings = [get_sentence_embedding(review, model) for review in batch_reviews]\n",
    "    embeddings_media.extend(batch_embeddings)\n",
    "X_embedding_media = np.array(embeddings_media, dtype=np.float32)\n",
    "X_embedding_media_sparse = scipy.sparse.csr_matrix(X_embedding_media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the lasso model on media data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Media Data (Normalized -1 to 1): 0.0962\n",
      "Root Mean Squared Error on Media Data (Normalized -1 to 1): 0.3102\n",
      "Transfer learning lasso kendall accuracy\n",
      "     acc  lower  upper\n",
      "0  68.12  67.65  68.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\34233\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# input X\n",
    "X_media_unique = scipy.sparse.hstack([X_tfidf_media,  X_ngrams_media, X_embedding_media_sparse])\n",
    "# normalize X\n",
    "scaler_x_filename = 'scaler_x.plk'\n",
    "scaler_x = pickle.load(open(scaler_x_filename, 'rb'))\n",
    "X_media_scaled = scaler_x.transform(X_media_unique)\n",
    "\n",
    "\n",
    "# predict\n",
    "y_pred_media_lasso = lasso.predict(X_media_scaled)\n",
    "\n",
    "# calculate mse and rmse\n",
    "mse_media = mean_squared_error(y_media_normalized, y_pred_media_lasso)\n",
    "print(f\"Mean Squared Error on Media Data (Normalized -1 to 1): {mse_media:.4f}\")\n",
    "rmse_media = np.sqrt(mse_media)\n",
    "print(f\"Root Mean Squared Error on Media Data (Normalized -1 to 1): {rmse_media:.4f}\")\n",
    "\n",
    "y_pred_media_original = scaler_y.inverse_transform(y_pred_media_lasso.reshape(-1, 1))\n",
    "y_media_original = scaler_y.inverse_transform(y_media_normalized)\n",
    "\n",
    "# kendall accuracy\n",
    "acc_transfer_lasso = kendall_acc(y_pred_media_original, y_media_original)\n",
    "print(\"Transfer learning lasso kendall accuracy\")\n",
    "print(acc_transfer_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new model on media review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and train set\n",
    "X_temp, _, y_train, y_test = train_test_split(media_df[[\"Clean_Snippet\"]],\n",
    "                                              media_df[\"Score\"],\n",
    "                                              test_size=0.2,\n",
    "                                              random_state=42)\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# normalized y\n",
    "y_train_normalized = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_normalized = scaler_y.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF feature\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, \n",
    "                                   ngram_range=(1, 3), \n",
    "                                   stop_words='english', \n",
    "                                   min_df=5, \n",
    "                                   max_df=0.95)\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_temp[\"Clean_Snippet\"])\n",
    "X_tfidf_test = tfidf_vectorizer.transform(media_df.loc[y_test.index, \"Clean_Snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram feature\n",
    "ngram_vectorizer = CountVectorizer(max_features=5000, \n",
    "                                   ngram_range=(1, 3), \n",
    "                                   stop_words='english', \n",
    "                                   min_df=5, \n",
    "                                   max_df=0.95)\n",
    "X_ngrams_train = ngram_vectorizer.fit_transform(X_temp[\"Clean_Snippet\"])\n",
    "X_ngrams_test = ngram_vectorizer.transform(media_df.loc[y_test.index, \"Clean_Snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of media reviwes\n",
    "def tokenize_reviews(reviews):\n",
    "    return [simple_preprocess(review) for review in reviews]\n",
    "\n",
    "tokenized_media_reviews = tokenize_reviews(media_df[\"Clean_Snippet\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding feature\n",
    "model = Word2Vec(sentences=tokenized_media_reviews, \n",
    "                 vector_size=100,\n",
    "                 window=5, \n",
    "                 min_count=5,\n",
    "                 workers=4)\n",
    "\n",
    "def get_sentence_embedding(review, model, vector_size=100):\n",
    "    words = simple_preprocess(review)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "batch_size = 10000\n",
    "embeddings_train = []\n",
    "for i in range(0, len(X_temp), batch_size):\n",
    "    batch_reviews = X_temp[\"Clean_Snippet\"].iloc[i:i + batch_size].tolist()\n",
    "    batch_embeddings = [get_sentence_embedding(review, model) for review in batch_reviews]\n",
    "    embeddings_train.extend(batch_embeddings)\n",
    "X_embedding_train = np.array(embeddings_train, dtype=np.float32)\n",
    "\n",
    "embeddings_test = []\n",
    "for i in range(0, len(y_test), batch_size):\n",
    "    batch_reviews = media_df.loc[y_test.index, \"Clean_Snippet\"].iloc[i:i + batch_size].tolist()\n",
    "    batch_embeddings = [get_sentence_embedding(review, model) for review in batch_reviews]\n",
    "    embeddings_test.extend(batch_embeddings)\n",
    "X_embedding_test = np.array(embeddings_test, dtype=np.float32)\n",
    "\n",
    "# build sparse matrix\n",
    "X_embedding_train_sparse = scipy.sparse.csr_matrix(X_embedding_train)\n",
    "X_embedding_test_sparse = scipy.sparse.csr_matrix(X_embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = scipy.sparse.hstack([X_tfidf_train, X_ngrams_train, X_embedding_train_sparse])\n",
    "X_test_combined = scipy.sparse.hstack([X_tfidf_test, X_ngrams_test, X_embedding_test_sparse])\n",
    "\n",
    "scaler_x = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_combined)\n",
    "X_test_scaled = scaler_x.transform(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "lasso = Lasso(alpha=0.001)  \n",
    "lasso.fit(X_train_scaled, y_train_normalized.flatten())\n",
    "\n",
    "# save model\n",
    "filename = 'lasso_model_media.sav'\n",
    "pickle.dump(lasso, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0110\n",
      "Root Mean Squared Error: 0.1048\n"
     ]
    }
   ],
   "source": [
    "lasso = pickle.load(open('lasso_model_media.sav','rb'))\n",
    "\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "mse_lasso = mean_squared_error(y_test_normalized, y_pred)\n",
    "print(f\"Mean Squared Error: {mse_lasso:.4f}\")\n",
    "\n",
    "rmse_lasso = np.sqrt(mse_lasso)\n",
    "print(f\"Root Mean Squared Error: {rmse_lasso:.4f}\")\n",
    "\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_names_ngrams = ngram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# important features\n",
    "important_features_meida = []\n",
    "for idx, coef in enumerate(lasso.coef_):\n",
    "    if abs(coef) > 0:\n",
    "        if idx < len(feature_names_tfidf):\n",
    "            important_features_meida.append((feature_names_tfidf[idx], coef))\n",
    "        elif idx < len(feature_names_tfidf)  + len(feature_names_ngrams):\n",
    "            important_features_meida.append((feature_names_ngrams[idx - len(feature_names_tfidf)], coef))\n",
    "        else:\n",
    "            important_features_meida.append((f\"Embedding_{idx - len(feature_names_tfidf)  - len(feature_names_ngrams)}\", coef))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Media Scores (0-100):\n",
      "[86.27726356 86.97206171 85.44281429 83.22377682 77.63991667 79.63552781\n",
      " 85.40482226 69.75211467 81.97213745 97.23802978]\n",
      "Actual Media Scores (0-100):\n",
      "[ 88.  80.  80.  75.  70.  75.  75.  60.  84. 100.]\n",
      "     acc  lower  upper\n",
      "0  74.88  73.91  75.85\n"
     ]
    }
   ],
   "source": [
    "y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "print(f\"Predicted Media Scores (0-100):\")\n",
    "print(y_pred_original.flatten()[:10])\n",
    "\n",
    "y_test_original = scaler_y.inverse_transform(y_test_normalized)\n",
    "print(f\"Actual Media Scores (0-100):\")\n",
    "print(y_test_original.flatten()[:10])\n",
    "\n",
    "# k acc\n",
    "acc_media_lasso = kendall_acc(y_pred_original, y_test_original)\n",
    "print(acc_media_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word  importance  frequency\n",
      "0        abil    0.000039   0.012552\n",
      "1      accept   -0.000073   0.002152\n",
      "2  accomplish    0.000796   0.005510\n",
      "3      achiev    0.001440   0.014182\n",
      "4      actual   -0.000243   0.013399\n",
      "5          ad    0.000170   0.014671\n",
      "6       admir   -0.000517   0.002641\n",
      "7       admit    0.000077   0.001337\n",
      "8        aegi    0.000045   0.001728\n",
      "9      afraid    0.000195   0.001761\n"
     ]
    }
   ],
   "source": [
    "# get all the features name\n",
    "all_feature_names_train = np.unique(np.concatenate([feature_names_tfidf, feature_names_ngrams]))\n",
    "vectorizer = CountVectorizer(vocabulary=all_feature_names_train)\n",
    "X_all_features = vectorizer.fit_transform(X_temp[\"Clean_Snippet\"])\n",
    "# calculate the features frequency\n",
    "word_doc_frequency = (X_all_features > 0).sum(axis=0) / X_all_features.shape[0]\n",
    "\n",
    "# convert frequency data to DataFrame\n",
    "word_freq_df = pd.DataFrame({\"word\": all_feature_names_train, \"doc_frequency\": np.array(word_doc_frequency).flatten()})\n",
    "word_freq_df = word_freq_df.sort_values(by=\"doc_frequency\", ascending=False)\n",
    "\n",
    "\n",
    "# merge the coefficient date to the frequency data\n",
    "important_features_df = pd.DataFrame(important_features_meida, columns=[\"word\", \"importance\"])\n",
    "word_freq_df = word_freq_df.rename(columns={\"doc_frequency\": \"frequency\"})\n",
    "freq_coef_media = pd.merge(important_features_df, word_freq_df, on=\"word\", how=\"inner\")\n",
    "\n",
    "# save data\n",
    "freq_coef_media.to_csv(\"freq_coef_media.csv\", index=False)\n",
    "print(freq_coef_media.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
